{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "432ba143",
   "metadata": {},
   "source": [
    "# OpenAI tests for lambda - azure function converter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe9ad9d",
   "metadata": {},
   "source": [
    "## Language Agent Tree Search Strategy\n",
    "\n",
    "The idea behind this notebook is to try to do the conversion by using the Language Agent Tree Search Strategy. The important point is to test the model by building the code and testing its results and in case things don't work, add a reflection and use it to iterate over the solution.\n",
    "\n",
    "The inspiration for this idea comes from the models evaluated in [Code Generation on HumanEval](https://paperswithcode.com/sota/code-generation-on-humaneval) specifically on the [LATS model](https://paperswithcode.com/paper/language-agent-tree-search-unifies-reasoning).\n",
    "\n",
    "Here's a nice explanation of the model: [LanguageAgentTreeSearch explanation](https://andyz245.github.io/LanguageAgentTreeSearch/)\n",
    "\n",
    "Here's a fork of their repo adding Azure and Golang support: [afrancoc2000 repo](https://github.com/afrancoc2000/LanguageAgentTreeSearch)\n",
    "\n",
    "Here's the original Dataset on HumanEval tests: [HumanEval](https://paperswithcode.com/dataset/humaneval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4caffe1",
   "metadata": {},
   "source": [
    "### Install the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc936208",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b1e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc97f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5584c760",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1621606",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6cc16f",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Import the libraries and environment variables to gain access to the `Open API Key`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd8d1ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://devsquad-eastus-2.openai.azure.com/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "from langchain.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "api_key=os.environ['OPENAI_API_KEY']\n",
    "base_url=os.environ['OPENAI_BASE_URL']\n",
    "api_version=os.environ['OPENAI_API_VERSION']\n",
    "\n",
    "print(base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3964d6",
   "metadata": {},
   "source": [
    "Let's create the llm client, we can use a chat llm from langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0af16d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=base_url, \n",
    "    api_version=api_version,\n",
    "    model=\"gpt-4\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305348e4",
   "metadata": {},
   "source": [
    "### Test input\n",
    "\n",
    "Let's define the target input that we'll use to select the examples and finally to run our query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64640ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_in_folders(path):\n",
    "    return os.listdir(path)\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7db03dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = read_file(\"../go-examplesexamples/test-inputs/basic-input-1/main.go\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc12f7b",
   "metadata": {},
   "source": [
    "### Using Advanced Chain-of-Thought (CoT) Prompt\n",
    "\n",
    "Add a system input that explains the steps the LLM needs to follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71a93ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_input = '''You are an AI that only responds with Go code, NOT ENGLISH. \n",
    "You will be given a lambda function code. \n",
    "Rewrite the code without using lambda code and using a GinGonic server instead.\n",
    "\n",
    "Use a Go code block to write your response. For example:\n",
    "\n",
    "```go\n",
    "func main() {{\n",
    "        fmt.Println(\\\"Hello, World!\\\")\n",
    "}}\n",
    "```\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c5d78d",
   "metadata": {},
   "source": [
    "### Create a Prompt template\n",
    "Configure prompt template with the system and user message to call the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcc77401",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_input),\n",
    "    (\"human\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d1fc79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an AI that only responds with Go code, NOT ENGLISH. \n",
      "You will be given a lambda function code. \n",
      "Rewrite the code without using lambda code and using a GinGonic server instead.\n",
      "\n",
      "Use a Go code block to write your response. For example:\n",
      "\n",
      "```go\n",
      "func main() {\n",
      "        fmt.Println(\"Hello, World!\")\n",
      "}\n",
      "```\n",
      "\n",
      "Human: package main\n",
      "\n",
      "import (\n",
      "\t\"context\"\n",
      "\t\"fmt\"\n",
      "\t\"github.com/aws/aws-lambda-go/lambda\"\n",
      "\t\"github.com/msft-latam-devsquad/lambda-to-azure-converter/examples/storage\"\n",
      ")\n",
      "\n",
      "type SaveRequest struct {\n",
      "\tId string `json:\"id\"`\n",
      "}\n",
      "\n",
      "type Response struct {\n",
      "\tMessage string `json:\"message\"`\n",
      "}\n",
      "\n",
      "func HandleRequest(ctx context.Context, req *SaveRequest) (*Response, error) {\n",
      "\tif req == nil {\n",
      "\t\treturn nil, fmt.Errorf(\"request can't be nil\")\n",
      "\t}\n",
      "\t\n",
      "\tazStore := storage.NewAzureStorage()\n",
      "\terr := azStore.Save(ctx, req.Id)\n",
      "\tif err != nil {\n",
      "\t\treturn nil, err\n",
      "\t}\n",
      "\n",
      "\tmessage := fmt.Sprintf(\"request %s was successfully saved\", req.Id)\n",
      "\treturn &Response{Message: message}, nil\n",
      "}\n",
      "\n",
      "func main() {\n",
      "\tlambda.Start(HandleRequest)\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(final_prompt.format(input=input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66abf00",
   "metadata": {},
   "source": [
    "### Use with an LLM\n",
    "Now, you can connect your model to the few-shot prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec6fb91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(llm=llm)\n",
    "\n",
    "result = conversation.run(final_prompt.format_messages(input=input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4201fad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```go\n",
      "package main\n",
      "\n",
      "import (\n",
      "\t\"context\"\n",
      "\t\"fmt\"\n",
      "\t\"github.com/gin-gonic/gin\"\n",
      "\t\"github.com/msft-latam-devsquad/lambda-to-azure-converter/examples/storage\"\n",
      "\t\"net/http\"\n",
      ")\n",
      "\n",
      "type SaveRequest struct {\n",
      "\tId string `json:\"id\" binding:\"required\"`\n",
      "}\n",
      "\n",
      "type Response struct {\n",
      "\tMessage string `json:\"message\"`\n",
      "}\n",
      "\n",
      "func saveHandler(c *gin.Context) {\n",
      "\tvar req SaveRequest\n",
      "\tif err := c.ShouldBindJSON(&req); err != nil {\n",
      "\t\tc.JSON(http.StatusBadRequest, gin.H{\"error\": err.Error()})\n",
      "\t\treturn\n",
      "\t}\n",
      "\n",
      "\tazStore := storage.NewAzureStorage()\n",
      "\terr := azStore.Save(context.Background(), req.Id)\n",
      "\tif err != nil {\n",
      "\t\tc.JSON(http.StatusInternalServerError, gin.H{\"error\": err.Error()})\n",
      "\t\treturn\n",
      "\t}\n",
      "\n",
      "\tmessage := fmt.Sprintf(\"request %s was successfully saved\", req.Id)\n",
      "\tc.JSON(http.StatusOK, &Response{Message: message})\n",
      "}\n",
      "\n",
      "func main() {\n",
      "\tr := gin.Default()\n",
      "\tr.POST(\"/save\", saveHandler)\n",
      "\tr.Run() // listen and serve on 0.0.0.0:8080 (for windows \"localhost:8080\")\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97621902",
   "metadata": {},
   "source": [
    "### Check if compiles \n",
    "\n",
    "The idea now is to programmatically check if the code is what we need so we need to create a temporal project, compile it and run unit tests to it to guarantee that the code is doing what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f275df4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"c:\\\\Workspace\\\\Americanas\\\\lambda-azfunction-converter-poc\\\\notebook\"\n",
    "\n",
    "def create_project() -> str:\n",
    "    pid = os.getpid()               # get id of the process\n",
    "    rand = os.urandom(8).hex()      # get random number\n",
    "    \n",
    "    temp_path = os.path.join(root_path, \"temp\")\n",
    "    temp_dir = f\"{temp_path}/go-lats-{pid}-{rand}\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    \n",
    "    # initialize a go project\n",
    "    os.chdir(temp_dir)\n",
    "    os.system(f\"go mod init go-lats-{pid}-{rand}\")\n",
    "    return temp_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba99d40",
   "metadata": {},
   "source": [
    "We need to write the file and fix any issues with imports, format and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3530f251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def write_to_file(path: str, code: str):\n",
    "    if not code.startswith(\"package \"):\n",
    "        code = f\"package main\\n\\n{code}\" \n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(code)\n",
    "\n",
    "def format_files(paths: List[str]):\n",
    "    for path in paths:\n",
    "        os.system(f\"go fmt {path}\")\n",
    "        os.system(f\"goimports -w {path}\")\n",
    "        os.system(\"go get -d ./...\")\n",
    "        os.system(\"go mod tidy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b50b1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = create_project()\n",
    "main_path = \"main.go\"\n",
    "\n",
    "code = result.replace(\"```go\\n\", \"\").replace(\"```\\n\", \"\")\n",
    "write_to_file(main_path, code)\n",
    "format_files([main_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dee495",
   "metadata": {},
   "source": [
    "Now, let's check if the code compiles and let's capture the output in case of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07eb4636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "import subprocess\n",
    "\n",
    "def run_process(cmd: str, tmp_path: str) -> Optional[Tuple[str, str]]:\n",
    "    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE,\n",
    "                         stderr=subprocess.PIPE, cwd=tmp_path)\n",
    "    out, err = p.communicate()\n",
    "    out = out.decode(\"utf-8\")\n",
    "    err = err.decode(\"utf-8\")\n",
    "    return out, err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b762c4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "buildResult = run_process(\"go build ./...\", temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ae63d6",
   "metadata": {},
   "source": [
    "Now we need to extract the errors as a list to be able to use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7f9689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompileErr:\n",
    "    def __init__(self, rendered):\n",
    "        self.rendered = rendered\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.rendered\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{\" + str(self) + \"}\"\n",
    "    \n",
    "def grab_compile_errs(inp: str) -> List[CompileErr]:\n",
    "    # we get a stream of json objects, so we need to parse them one by one\n",
    "    objs = []\n",
    "    compileErr = \"\"\n",
    "    for line in inp.splitlines():\n",
    "        if line == \"\":\n",
    "            continue\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "        if line.startswith(\".\\\\lats.go\"):\n",
    "            if compileErr != \"\":\n",
    "                objs.append(CompileErr(compileErr))\n",
    "            compileErr = line.strip() + \"\\n\"\n",
    "        if line.startswith(\"        \"):\n",
    "            compileErr += line.strip() + \"\\n\"\n",
    "    \n",
    "    if compileErr != \"\":\n",
    "        objs.append(CompileErr(compileErr))\n",
    "\n",
    "    return objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58c78159",
   "metadata": {},
   "outputs": [],
   "source": [
    "errs = grab_compile_errs(buildResult[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4b07c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(errs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efefb22c",
   "metadata": {},
   "source": [
    "### Generate unit tests for the code\n",
    "\n",
    "Another important feedback to know if our transformation works is going to be unit tests, even functional tests if they are available, so let's ask the LLM to generate some for us"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf8a038",
   "metadata": {},
   "source": [
    "Let's try to get some feedback about what the input is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "477b1028",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_input = '''\n",
    "You are a Go programming assistant, an AI coding assistant that generates the comments for a function \n",
    "in the code you receive, to describe what it does and how it works. you include a description of the inputs\n",
    "and outputs of the function, and any other relevant information.\n",
    "\n",
    "You return the code enclosed in a code block, and include the signature of the function.\n",
    "\n",
    "You do not return English, only Go code.\n",
    "\n",
    "For example:\n",
    "\n",
    "Given the following code:\n",
    "```go\n",
    "func main() {{\n",
    "        fmt.Println(\\\"Hello, World!\\\")\n",
    "}}\n",
    "```\n",
    "\n",
    "return:\n",
    "```go\n",
    "// main prints Hello, World! to the console\n",
    "func main() {{\n",
    "```\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a103025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", qa_system_input),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "conversation = ConversationChain(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9ff988d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```go\n",
      "// HandleRequest takes a context and a SaveRequest, saves the request ID to Azure storage,\n",
      "// and returns a Response with a success message or an error if the save operation fails.\n",
      "func HandleRequest(ctx context.Context, req *SaveRequest) (*Response, error) {\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "purpose = conversation.run(\n",
    "    qa_prompt.format_messages(input=f\"Generate the comment for the handler in this code \\n\\n{input}\"))\n",
    "print(purpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af570a9f",
   "metadata": {},
   "source": [
    "And now with the purpose and the handler signature let's generate the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "92597c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_system_input = '''\n",
    "You are a Go programming assistant, an AI coding assistant that can write unique, diverse, and intuitive \n",
    "unit tests for functions given the target code.\n",
    "\n",
    "For example:\n",
    "\n",
    "func signature:\n",
    "/// Add three numbers together.\n",
    "/// This function takes three numbers as input and returns the sum of the three numbers.\n",
    "func Add3Numbers(x int, y int, z int) int {{\n",
    "\n",
    "unit tests:\n",
    "```go\t\n",
    "func TestAdd(t *testing.T) {{\n",
    "    assert := assert.New(t)\n",
    "    assert.Equal(7, Add3Numbers(2, 3+rand.Intn(1000)*0, 2))\n",
    "    assert.Equal(15, Add3Numbers(5, 7, 3))\n",
    "}}\n",
    "```\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "715b7ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", test_system_input),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "conversation = ConversationChain(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "53219c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```go\n",
      "func TestHandleRequest(t *testing.T) {\n",
      "    assert := assert.New(t)\n",
      "    ctx := context.Background()\n",
      "    req := &SaveRequest{RequestID: \"test-id\"}\n",
      "\n",
      "    // Mock the Azure storage save operation here\n",
      "    // You would typically use a mocking framework or a stub for the actual implementation\n",
      "\n",
      "    // Test for a successful save operation\n",
      "    response, err := HandleRequest(ctx, req)\n",
      "    assert.Nil(err)\n",
      "    assert.NotNil(response)\n",
      "    assert.Equal(\"Request saved successfully\", response.Message)\n",
      "\n",
      "    // Test for a failed save operation\n",
      "    // Here you would simulate a failure in saving to Azure storage\n",
      "    response, err = HandleRequest(ctx, req)\n",
      "    assert.NotNil(err)\n",
      "    assert.Nil(response)\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "tests = conversation.run(\n",
    "    test_prompt.format_messages(input=purpose))\n",
    "print(tests)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
